{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n#!pip install numpy==1.17.5\n#!pip install -U numpy==1.11.0\n!pip3 install numpy==1.17.4\n\nimport numpy as np\nimport pandas as pd \nfrom tqdm import tqdm\n\nfrom matplotlib import image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport cv2\n\n!pip install fastai --upgrade\n\nimport torch \nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\nfrom fastai.vision.all import *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-20T01:51:05.634219Z","iopub.execute_input":"2021-10-20T01:51:05.634612Z","iopub.status.idle":"2021-10-20T01:52:52.402777Z","shell.execute_reply.started":"2021-10-20T01:51:05.634528Z","shell.execute_reply":"2021-10-20T01:52:52.401563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://drive.google.com/u/0/open?id=1a2oHjcEcwXP8oUF95qiwrqzACb2YlUhn","metadata":{"execution":{"iopub.status.busy":"2021-10-20T02:23:13.675097Z","iopub.execute_input":"2021-10-20T02:23:13.675485Z","iopub.status.idle":"2021-10-20T02:23:15.160691Z","shell.execute_reply.started":"2021-10-20T02:23:13.675441Z","shell.execute_reply":"2021-10-20T02:23:15.159454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sequences from day 1","metadata":{}},{"cell_type":"code","source":"plt_image = image.imread('../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00000.jpg')\nprint(plt_image.dtype)\nprint(plt_image.shape)\nplt.imshow(plt_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:52.407694Z","iopub.execute_input":"2021-10-20T01:52:52.409827Z","iopub.status.idle":"2021-10-20T01:52:52.762506Z","shell.execute_reply.started":"2021-10-20T01:52:52.409787Z","shell.execute_reply":"2021-10-20T01:52:52.761654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt_image = image.imread('../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00900.jpg')\nprint(plt_image.dtype)\nprint(plt_image.shape)\nplt.imshow(plt_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:52.764131Z","iopub.execute_input":"2021-10-20T01:52:52.764447Z","iopub.status.idle":"2021-10-20T01:52:53.024069Z","shell.execute_reply.started":"2021-10-20T01:52:52.764399Z","shell.execute_reply":"2021-10-20T01:52:53.023284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt_image = image.imread('../input/lisa-traffic-light-dataset/nightSequence1/nightSequence1/frames/nightSequence1--00000.jpg')\nprint(plt_image.dtype)\nprint(plt_image.shape)\nplt.imshow(plt_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:53.025607Z","iopub.execute_input":"2021-10-20T01:52:53.026079Z","iopub.status.idle":"2021-10-20T01:52:53.28772Z","shell.execute_reply.started":"2021-10-20T01:52:53.026032Z","shell.execute_reply":"2021-10-20T01:52:53.28672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt_image = image.imread('../input/lisa-traffic-light-dataset/nightSequence1/nightSequence1/frames/nightSequence1--00100.jpg')\nprint(plt_image.dtype)\nprint(plt_image.shape)\nplt.imshow(plt_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:53.28921Z","iopub.execute_input":"2021-10-20T01:52:53.28964Z","iopub.status.idle":"2021-10-20T01:52:53.544456Z","shell.execute_reply.started":"2021-10-20T01:52:53.289603Z","shell.execute_reply":"2021-10-20T01:52:53.543512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day1_frame_annotations_BOX = pd.read_csv(\"../input/lisa-traffic-light-dataset/Annotations/Annotations/daySequence1/frameAnnotationsBOX.csv\", sep=';')\nday1_frame_annotations_BULB = pd.read_csv(\"../input/lisa-traffic-light-dataset/Annotations/Annotations/daySequence1/frameAnnotationsBULB.csv\", sep=';')\nnight1_frame_annotations_BOX = pd.read_csv(\"../input/lisa-traffic-light-dataset/Annotations/Annotations/nightSequence1/frameAnnotationsBOX.csv\", sep=';')\nnight1_frame_annotations_BULB = pd.read_csv(\"../input/lisa-traffic-light-dataset/Annotations/Annotations/nightSequence1/frameAnnotationsBULB.csv\", sep=';')\nday2_frame_annotations_BOX = pd.read_csv(\"../input/lisa-traffic-light-dataset/Annotations/Annotations/daySequence2/frameAnnotationsBOX.csv\", sep=';')\nday2_frame_annotations_BULB = pd.read_csv(\"../input/lisa-traffic-light-dataset/Annotations/Annotations/daySequence2/frameAnnotationsBULB.csv\", sep=';')\n\nday2_frame_annotations_BOX.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:53.545969Z","iopub.execute_input":"2021-10-20T01:52:53.546329Z","iopub.status.idle":"2021-10-20T01:52:53.885071Z","shell.execute_reply.started":"2021-10-20T01:52:53.546288Z","shell.execute_reply":"2021-10-20T01:52:53.884246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#frame_annotations_BULB.iloc[0]['']\n\nplt_image = image.imread('../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00000.jpg')\n\nfig, ax = plt.subplots()\n\nax.imshow(plt_image)\n\n# Create a Rectangle patch\nrect = patches.Rectangle((710, 481), 714-710, 486-481, linewidth=1, edgecolor='r', facecolor='none')\n\nax.add_patch(rect)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:53.886475Z","iopub.execute_input":"2021-10-20T01:52:53.88684Z","iopub.status.idle":"2021-10-20T01:52:54.143514Z","shell.execute_reply.started":"2021-10-20T01:52:53.886805Z","shell.execute_reply":"2021-10-20T01:52:54.142303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pipeline","metadata":{}},{"cell_type":"code","source":"def center_format(df):\n    cols = ['name', 'center_x', 'center_y', 'height', 'width', 'tag']\n    tags = {'stopLeft': 0, 'go': 1, 'warningLeft': 2, 'stop': 0, 'warning': 2, 'goLeft': 1, 'goForward': 1}\n    tags_idx = 0\n\n    boxes_df = pd.DataFrame(columns=cols)\n    for row in tqdm(df.iterrows()):\n        center_x = (row[1]['Lower right corner X'] + row[1]['Upper left corner X'])/2\n        center_y = (row[1]['Lower right corner Y'] + row[1]['Upper left corner Y'])/2\n        height = row[1]['Lower right corner Y'] - row[1]['Upper left corner Y']\n        width = row[1]['Lower right corner X'] - row[1]['Upper left corner X']\n        tag = tags[row[1]['Annotation tag']]\n    \n        name = row[1]['Filename']\n        data_dict = {'name': name, 'center_x': center_x , 'center_y': center_y, 'height': height, 'width': width, 'tag': tag}\n        boxes_df.loc[row[0]] = pd.Series(data_dict)\n        \n    return boxes_df\n\n\ndef min_max_box_format(df):\n    cols = ['name', 'x_min', 'y_min', 'x_max', 'y_max', 'class_id']\n    tags = {'stopLeft': 0, 'go': 1, 'warningLeft': 2, 'stop': 0, 'warning': 2, 'goLeft': 1, 'goForward': 1}\n    tags_idx = 0\n\n    boxes_df = pd.DataFrame(columns=cols)\n    for row in tqdm(df.iterrows()):\n        min_x = row[1]['Upper left corner X']\n        min_y = row[1]['Upper left corner Y'] \n        max_y = row[1]['Lower right corner Y']\n        max_x = row[1]['Lower right corner X'] \n        tag = tags[row[1]['Annotation tag']]\n        name = row[1]['Filename']\n        data_dict = {'name': name, 'x_min': min_x , 'y_min': min_y, 'x_max': max_x, 'y_max': max_y, 'class_id': tag}\n        boxes_df.loc[row[0]] = pd.Series(data_dict)\n    return boxes_df","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:54.146293Z","iopub.execute_input":"2021-10-20T01:52:54.146646Z","iopub.status.idle":"2021-10-20T01:52:54.158763Z","shell.execute_reply.started":"2021-10-20T01:52:54.146602Z","shell.execute_reply":"2021-10-20T01:52:54.157503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#min_max_boxes_df_day1_BULB = min_max_box_format(day1_frame_annotations_BULB)\nmin_max_boxes_df_day1_BOX = min_max_box_format(day1_frame_annotations_BOX)\n#min_max_boxes_df_night1_BULB = min_max_box_format(night1_frame_annotations_BULB)\nmin_max_boxes_df_day2_BOX = min_max_box_format(day2_frame_annotations_BOX)\nmin_max_boxes_df_day2_BOX","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:52:54.161082Z","iopub.execute_input":"2021-10-20T01:52:54.161496Z","iopub.status.idle":"2021-10-20T01:54:09.911328Z","shell.execute_reply.started":"2021-10-20T01:52:54.161438Z","shell.execute_reply":"2021-10-20T01:54:09.910417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = len(np.unique(min_max_boxes_df_day2_BOX['class_id'])) + 1 # 3 classes (Stop, Warning, Go) + Background\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:09.913053Z","iopub.execute_input":"2021-10-20T01:54:09.913567Z","iopub.status.idle":"2021-10-20T01:54:09.930947Z","shell.execute_reply.started":"2021-10-20T01:54:09.913506Z","shell.execute_reply":"2021-10-20T01:54:09.929821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:09.932466Z","iopub.execute_input":"2021-10-20T01:54:09.933184Z","iopub.status.idle":"2021-10-20T01:54:10.974667Z","shell.execute_reply.started":"2021-10-20T01:54:09.933144Z","shell.execute_reply":"2021-10-20T01:54:10.973809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_image(img_arr, bboxes, h, w):\n    \"\"\"\n    :param img_arr: original image as a numpy array\n    :param bboxes: bboxes as numpy array where each row is 'x_min', 'y_min', 'x_max', 'y_max', \"class_id\"\n    :param h: resized height dimension of image\n    :param w: resized weight dimension of image\n    :return: dictionary containing {image:transformed, bboxes:['x_min', 'y_min', 'x_max', 'y_max', \"class_id\"]}\n    \"\"\"\n    # create resize transform pipeline\n    transform = albumentations.Compose(\n        [albumentations.Resize(height=h, width=w, always_apply=True)],\n        bbox_params=albumentations.BboxParams(format='pascal_voc'))\n\n    transformed = transform(image=img_arr, bboxes=bboxes)\n\n    return transformed","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:10.976015Z","iopub.execute_input":"2021-10-20T01:54:10.976377Z","iopub.status.idle":"2021-10-20T01:54:10.983112Z","shell.execute_reply.started":"2021-10-20T01:54:10.976339Z","shell.execute_reply":"2021-10-20T01:54:10.982311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox_cols = ['x_min', 'y_min', 'x_max', 'y_max']\n\ndef get_target_ds(name, df):  \n    rows = df[df[\"name\"] == 'dayTest/'+name[-23:]]  \n    bbox = []\n    for row, class_id in zip(rows[bbox_cols].values, rows[\"class_id\"].values):\n        bbox_entry = []\n        for box in row:\n            bbox_entry.append(box)\n        bbox_entry.append(class_id)\n        bbox.append(bbox_entry)\n    return bbox\n\nclass LisaDataset(torch.utils.data.Dataset):  \n    def __init__(self, images_path, df, std=False):\n        super(LisaDataset, self).__init__()\n        self.images_path = images_path\n        self.df = df\n        self.std = std\n        \n    def __len__(self):\n        return len(self.images_path)  \n\n    def __getitem__(self, idx):\n        img_path = self.images_path[idx]\n        img = cv2.imread(str(img_path), cv2.IMREAD_UNCHANGED)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        \n        boxes = get_target_ds(img_path.name, self.df)\n            \n        transformed_dict = resize_image(img, boxes, 256, 256)\n\n        # contains the image as array\n        img = transformed_dict[\"image\"]\n\n        # contains the resized bounding boxes\n        boxes_with_labels = np.array(list(map(list, transformed_dict[\"bboxes\"]))).astype(float)\n        \n        if self.std:\n            img = img/255.0\n            \n        img = img.reshape(3, 256, 256)\n        #img = cv2.resize(img, (512, 512))\n        target = {}\n        \n        if len(boxes_with_labels.shape) == 1:\n            if boxes_with_labels.shape[0] == 0:\n                boxes = np.array([[], [], [], []]).T\n                labels = torch.as_tensor(np.array([]), dtype=torch.int64)\n                areas = np.array([])\n            else:\n                boxes = boxes_with_labels[:-1].astype(np.int64)\n                labels = torch.as_tensor(boxes_with_labels[-1].astype(np.float), dtype=torch.int64)\n                areas = (boxes[3] - boxes[1]) * (boxes[2] - boxes[0])\n        else:\n            boxes = boxes_with_labels[:, :-1].astype(np.int64)\n            labels = torch.as_tensor(boxes_with_labels[:, -1].astype(np.float), dtype=torch.int64)\n            areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            \n        iscrowd = torch.zeros((boxes.shape[0],))\n        image_id = torch.tensor([idx])\n        areas = torch.as_tensor(areas.astype(np.float), dtype=torch.double)\n        boxes = torch.from_numpy(boxes) \n        boxes = torch.as_tensor(boxes, dtype=torch.int64)\n        \n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = areas\n        target[\"iscrowd\"] = iscrowd\n        target[\"image_id\"] = image_id    \n        img = torch.from_numpy(img)\n        img = torch.as_tensor(img, dtype=torch.double)\n        \n        return img, target","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:10.984593Z","iopub.execute_input":"2021-10-20T01:54:10.985137Z","iopub.status.idle":"2021-10-20T01:54:11.004816Z","shell.execute_reply.started":"2021-10-20T01:54:10.98509Z","shell.execute_reply":"2021-10-20T01:54:11.003848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we read the day 1 sequence images","metadata":{}},{"cell_type":"code","source":"day_sequence_1 = Path(\"../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/\") \nday_sequence_1_images = get_image_files(day_sequence_1)\nday_sequence_2 = Path(\"../input/lisa-traffic-light-dataset/daySequence2/daySequence2/frames/\") \nday_sequence_2_images = get_image_files(day_sequence_2)\n\nimg = cv2.imread(str(day_sequence_1_images[-1]), cv2.IMREAD_UNCHANGED)\nimg = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:11.006433Z","iopub.execute_input":"2021-10-20T01:54:11.007133Z","iopub.status.idle":"2021-10-20T01:54:22.112209Z","shell.execute_reply.started":"2021-10-20T01:54:11.007097Z","shell.execute_reply":"2021-10-20T01:54:22.108879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = LisaDataset(day_sequence_1_images, min_max_boxes_df_day1_BOX)\nimg, target = dataset.__getitem__(0)\nimg = img.reshape(256, 256, 3)\nimg = torch.as_tensor(img, dtype=torch.int)\nprint(img.shape, target[\"boxes\"], target[\"labels\"])\n\nfig, ax = plt.subplots(figsize=(20,20))\nax.imshow(img)\nrect = patches.Rectangle((76, 199), 87-76, 223-199, linewidth=1, edgecolor='r', facecolor='none')\nax.add_patch(rect)\nrect = patches.Rectangle((241, 166), 247-241, 180-166, linewidth=1, edgecolor='r', facecolor='none')\nax.add_patch(rect)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:22.113515Z","iopub.execute_input":"2021-10-20T01:54:22.113873Z","iopub.status.idle":"2021-10-20T01:54:22.423676Z","shell.execute_reply.started":"2021-10-20T01:54:22.113828Z","shell.execute_reply":"2021-10-20T01:54:22.419855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build the Model","metadata":{}},{"cell_type":"code","source":"if(torch.cuda.is_available()):\n    device = torch.device(\"cuda\")\n    print(\"Device:\", device, torch.cuda.get_device_name(0))\nelse:\n    device= torch.device(\"cpu\")\n    print(\"Device:\", device)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:22.424946Z","iopub.execute_input":"2021-10-20T01:54:22.425286Z","iopub.status.idle":"2021-10-20T01:54:22.43213Z","shell.execute_reply.started":"2021-10-20T01:54:22.425244Z","shell.execute_reply":"2021-10-20T01:54:22.430952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"build the dataloader","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:22.433546Z","iopub.execute_input":"2021-10-20T01:54:22.433892Z","iopub.status.idle":"2021-10-20T01:54:22.441029Z","shell.execute_reply.started":"2021-10-20T01:54:22.433857Z","shell.execute_reply":"2021-10-20T01:54:22.439946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%sh\nif [ ! -d \"output/vision\" ]; then\n    git clone https://github.com/pytorch/vision.git output/vision\nelse\n    echo \"output/vision already cloned\"\nfi\n\ncp output/vision/references/detection/utils.py .\ncp output/vision/references/detection/transforms.py .\ncp output/vision/references/detection/coco_eval.py .\ncp output/vision/references/detection/engine.py .\ncp output/vision/references/detection/coco_utils.py .","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:22.442537Z","iopub.execute_input":"2021-10-20T01:54:22.443034Z","iopub.status.idle":"2021-10-20T01:54:33.185329Z","shell.execute_reply.started":"2021-10-20T01:54:22.442996Z","shell.execute_reply":"2021-10-20T01:54:33.184337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip3 install pycocotools\n!pip3 install -U scikit-image\n!pip3 install -U cython \n#!pip3 install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:54:33.18705Z","iopub.execute_input":"2021-10-20T01:54:33.187403Z","iopub.status.idle":"2021-10-20T01:55:08.105647Z","shell.execute_reply.started":"2021-10-20T01:54:33.187362Z","shell.execute_reply":"2021-10-20T01:55:08.104572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfasterRCNN_model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\nin_features = fasterRCNN_model.roi_heads.box_predictor.cls_score.in_features\nfasterRCNN_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:57:49.282204Z","iopub.execute_input":"2021-10-20T01:57:49.282633Z","iopub.status.idle":"2021-10-20T01:57:50.418563Z","shell.execute_reply.started":"2021-10-20T01:57:49.282597Z","shell.execute_reply":"2021-10-20T01:57:50.417636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LossAverager:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:57:56.434263Z","iopub.execute_input":"2021-10-20T01:57:56.434652Z","iopub.status.idle":"2021-10-20T01:57:56.440896Z","shell.execute_reply.started":"2021-10-20T01:57:56.434616Z","shell.execute_reply":"2021-10-20T01:57:56.43961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"day_sequence_1_dataset = LisaDataset(day_sequence_1_images, min_max_boxes_df_day1_BOX, True)\nday_sequence_2_dataset = LisaDataset(day_sequence_2_images, min_max_boxes_df_day2_BOX, True)\n    \n# split the dataset in train and test set\n#indices = torch.randperm(len(day_sequence_1_dataset)).tolist()\n#day_sequence_1_dataset = torch.utils.data.Subset(day_sequence_1_dataset, indices[:-1000])\n#day_sequence_1_dataset_val = torch.utils.data.Subset(day_sequence_1_dataset, indices[-1000:])\n\n    # define training and validation data loaders\nday_sequence_1_dataloader = torch.utils.data.DataLoader(\n        day_sequence_1_dataset, batch_size=2, shuffle=True, num_workers=0,\n        collate_fn = collate_fn)\n\nday_sequence_2_dataloader = torch.utils.data.DataLoader(\n        day_sequence_2_dataset, batch_size=1, shuffle=False, num_workers=0,\n        collate_fn = collate_fn)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:58:00.5668Z","iopub.execute_input":"2021-10-20T01:58:00.567149Z","iopub.status.idle":"2021-10-20T01:58:00.57228Z","shell.execute_reply.started":"2021-10-20T01:58:00.567116Z","shell.execute_reply":"2021-10-20T01:58:00.57144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport time\n\ntrain_loss_hist = LossAverager()\nval_loss_hist = LossAverager()\nEPOCHS = 10\n\nfasterRCNN_model.double()\nfasterRCNN_model.to(device)\n\nparams = [p for p in fasterRCNN_model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params)#, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=3,\n                                                   gamma=0.1)\n\n\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    start_time = time.time()\n    fasterRCNN_model.train()\n    train_loss_hist.reset()\n    \n    for images, targets in tqdm(day_sequence_1_dataloader):\n        \n        images = torch.stack(images).to(device)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        bs = images.shape[0]\n        \n        loss_dict = fasterRCNN_model(images, targets)\n        \n        totalLoss = sum(loss for loss in loss_dict.values())\n        lossValue = totalLoss.item()\n        \n        train_loss_hist.update(lossValue,bs)\n\n        optimizer.zero_grad()\n        totalLoss.backward()\n        optimizer.step()\n        lr_scheduler.step(totalLoss)\n        \n    print(f\"Train loss: {train_loss_hist.avg}\")\n    evaluate(fasterRCNN_model, day_sequence_1_dataloader, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T01:58:08.775671Z","iopub.execute_input":"2021-10-20T01:58:08.775995Z","iopub.status.idle":"2021-10-20T02:06:33.092781Z","shell.execute_reply.started":"2021-10-20T01:58:08.775964Z","shell.execute_reply":"2021-10-20T02:06:33.091367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}